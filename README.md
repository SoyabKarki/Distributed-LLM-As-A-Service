# Distributed LLM-As-A-Service

This is an ONGOING project. The idea is to host an LLM inference backend that is distributed across pods in a Kubernetes cluster along with load balancing and API Gateways to enable horizontal scalability. This backend will be hooked to a frontend with a ChatGPT-like UI.  
  
Future goals include transferring the codebase to the cloud (probably GCP).  
