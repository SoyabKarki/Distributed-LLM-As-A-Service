apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend
  namespace: llm-as-a-service
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend 
  minReplicas: 2  # Minimum number of replicas to scale up to
  maxReplicas: 4  # Maximum number of replicas to scale up to
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50  # Target 50% CPU utilization