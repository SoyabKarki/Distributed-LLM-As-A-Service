apiVersion: apps/v1
kind: Deployment
metadata: 
  name: backend
  namespace: llm-as-a-service 
spec:
  replicas: 2  # 2 replicas to run
  selector:
    matchLabels: 
      app: backend  # Match the label in the pod template
  template:
    metadata: 
      labels:
        app: backend
    spec:
      containers:
        - name: backend
          image: distributed-llm-as-a-service-backend:latest  # Use the image built from the Dockerfile in the backend directory
          imagePullPolicy: Never  # Don't pull the image from a registry, use the locally built image
          ports:
            - containerPort: 8000  # Port the container listens on
          envFrom:
            - configMapRef:
                name: backend-config  # Use the config map to set environment variables
          resources:
            requests:
              cpu: "100m"  # 100 millicores = 0.1 CPU
              memory: "128Mi" # 128 MB of memory
            limits:
              cpu: "500m" # 0.5 CPU
              memory: "256Mi" # 256 MB of memory
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 10  # Wait 10 seconds before starting the probe
            periodSeconds: 30  # Check every 30 seconds
            timeoutSeconds: 5  # Timeout after 5 seconds
            failureThreshold: 3  # Restart pod after 3 consecutive failures
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 5  # Wait 5 seconds before starting the probe
            periodSeconds: 10  # Check every 10 seconds
            timeoutSeconds: 5  # Timeout after 5 seconds
            failureThreshold: 3  # Restart pod after 3 consecutive failures